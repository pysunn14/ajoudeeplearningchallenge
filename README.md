# ì•„ì£¼ì†Œì¤‘í•œë”¥ëŸ¬ë‹ì±Œë¦°ì§€ 

## Environment

- Python 3.11.13
- pip : requirements.txt
- conda : environment.yml
- GPU : A5000 24GB
- CUDA Version: 12.6

### í´ë” êµ¬ì¡° 

<img width="366" height="691" alt="image" src="https://github.com/user-attachments/assets/2377d1ee-1a5a-4fec-a7fb-1b2b6ef7a462" />


- ì†ŒìŠ¤ì½”ë“œ ëŒ€ë¶€ë¶„ ì ˆëŒ€ê²½ë¡œë¡œ ì‘ì—…í•˜ì—¬ ê²½ë¡œ ë³€í™˜ í•„ìš”

model_download.py : Qwen2.5VL ë‹¤ìš´ë¡œë“œ
download_image.py : ì´ë¯¸ì§€ ìºì‹œ ì €ì¥ 
preprocess_train.py : í›ˆë ¨ ë°ì´í„°ì…‹ ì „ì²˜ë¦¬
preprocess_inference.py : ì¶”ë¡  ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ 

train_model.py : finetuning ëª¨ë¸ í›ˆë ¨
inference.py : ëª¨ë¸ ì¶”ë¡  ì§„í–‰ 

# í›ˆë ¨ ì˜µì…˜ 

## preprocess_train --mode [multimodal, text]

- text only ëª¨ë“œë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤ 
- í˜„ì¬ í›ˆë ¨ ë° ì¶”ë¡ ì´ text-only ë¡œ ì§„í–‰í•˜ì˜€ìœ¼ë¯€ë¡œ multimodal ì‚¬ìš© X

## train_model.py



## inference.py 

--base 
--adapter ì–´ëŒ‘í„° ê²½ë¡œë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. finetuned/checkpoint-338 ë“± 
--sample ìƒ˜í”Œ ë°ì´í„°ì…‹ìœ¼ë¡œ ì¶”ë¡ ì„ ì§„í–‰í•©ë‹ˆë‹¤. 

## evaluation.py

-- Ground Truth ì—­í• ì„ í•˜ëŠ” GT.csv ë¥¼ ìƒì„± í›„ ìƒ˜í”Œ ì…‹ì— ëŒ€í•œ ì¶”ë¡  ê²°ê³¼ ê²€ì¦ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. 
# Ajou Multimodal Deep Learning Challenge

> ì•„ì£¼ì†Œì¤‘í•œë”¥ëŸ¬ë‹ì±Œë¦°ì§€ ê²½ì§„ëŒ€íšŒ - ë©€í‹°ëª¨ë‹¬ AI ëª¨ë¸ ê°œë°œ 
> Qwen2.5-VL 7B ëª¨ë¸ ê¸°ë°˜ multitask í•™ìŠµ ì‹œìŠ¤í…œ

[![Python 3.11](https://img.shields.io/badge/Python-3.11-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![CUDA](https://img.shields.io/badge/CUDA-12.6-green.svg)](https://developer.nvidia.com/cuda-downloads)

## í”„ë¡œì íŠ¸ êµ¬ì¡°

```
multimodal/
â”œâ”€â”€ src/                          # ì†ŒìŠ¤ ì½”ë“œ
â”‚   â”œâ”€â”€ model_download.py         # Qwen2.5-VL ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
â”‚   â”œâ”€â”€ download_images.py        # ì´ë¯¸ì§€ ìºì‹œ ì €ì¥ ì‹œìŠ¤í…œ
â”‚   â”œâ”€â”€ preprocess_train.py       # í›ˆë ¨ ë°ì´í„° ì „ì²˜ë¦¬
â”‚   â”œâ”€â”€ preprocess_inference.py   # ì¶”ë¡  ë°ì´í„° ì „ì²˜ë¦¬  
â”‚   â”œâ”€â”€ train_model.py           # ëª¨ë¸ íŒŒì¸íŠœë‹ í›ˆë ¨
â”‚   â”œâ”€â”€ inference.py             # ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰
â”‚   â”œâ”€â”€ finetuned/              # í›ˆë ¨ëœ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ 
â”‚   â””â”€â”€ submissions/            # ì œì¶œ íŒŒì¼ ì €ì¥ì†Œ
â”œâ”€â”€ dataset/deeplearningchallenge # ë°ì´í„°ì…‹ 
â”œâ”€â”€ image_cache/                # ì´ë¯¸ì§€ ìºì‹œ 
â”œâ”€â”€ download_report/            # ë‹¤ìš´ë¡œë“œ ë¦¬í¬íŠ¸ 
â”œâ”€â”€ environment.yml             # Conda í™˜ê²½ ì„¤ì •
â”œâ”€â”€ requirements.txt            # pip íŒ¨í‚¤ì§€ ëª©ë¡
â””â”€â”€ README.md                   # í”„ë¡œì íŠ¸ ë¬¸ì„œ
```

![í”„ë¡œì íŠ¸ êµ¬ì¡°](image.png)

## ğŸ› ï¸ í™˜ê²½ ì„¤ì •

### ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­
- **Python**: 3.11.13
- **GPU**: NVIDIA A5000 24GB (ê¶Œì¥)
- **CUDA**: 12.6
- **RAM**: 24GB+ (ê¶Œì¥)

### ì„¤ì¹˜ ë°©ë²•

- Datasetì€ ì‚¬ì „ì— ë‹¤ìš´í•˜ì—¬ ì €ì¥í•´ì•¼í•©ë‹ˆë‹¤. 

#### Option 1: Conda í™˜ê²½ (ê¶Œì¥)
```bash
# í™˜ê²½ ìƒì„± ë° í™œì„±í™”
conda env create -f environment.yml
conda activate mlenv
```

#### Option 2: pip ì„¤ì¹˜
```bash
# ê°€ìƒí™˜ê²½ ìƒì„±
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate   # Windows

# íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install -r requirements.txt
```

## ì‚¬ìš©ë²•

### 1. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
```bash
python src/model_download.py
```

### 2. ì´ë¯¸ì§€ ìºì‹œ êµ¬ì¶•
```bash
python src/download_images.py
```

### 3. ë°ì´í„° ì „ì²˜ë¦¬

#### í›ˆë ¨ ë°ì´í„° ì „ì²˜ë¦¬
```bash
# í…ìŠ¤íŠ¸ ì „ìš© ëª¨ë“œ (í•„ìˆ˜)
python src/preprocess_train.py --mode text

# ë©€í‹°ëª¨ë‹¬ ëª¨ë“œ (í›ˆë ¨ ë° ì¶”ë¡  í”„ë¡œì„¸ìŠ¤ê°€ text-onlyë¡œ ì§„í–‰í–ˆìœ¼ë¯€ë¡œ, ì‚¬ìš© X)
python src/preprocess_train.py --mode multimodal
```

#### ì¶”ë¡  ë°ì´í„° ì „ì²˜ë¦¬
```bash
# 3-shot í”„ë¡¬í”„íŠ¸ (ê¸°ë³¸ê°’)
python src/preprocess_inference.py --mode three

# 1-shot ë˜ëŠ” 2-shot í”„ë¡¬í”„íŠ¸
python src/preprocess_inference.py --mode one
python src/preprocess_inference.py --mode two

# ìƒ˜í”Œ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
python src/preprocess_inference.py --sample --mode three
```

- one shot ì„±ëŠ¥ì´ ê°€ì¥ ë†’ê²Œ ë‚˜ì˜µë‹ˆë‹¤ 

### 4. ëª¨ë¸ í›ˆë ¨
```bash
# ê¸°ë³¸ í›ˆë ¨ (2 ì—í­)
python src/train_model.py

# ì‚¬ìš©ì ì •ì˜ ì„¤ì •
python src/train_model.py --epochs 3 
```

#### í›ˆë ¨ ì„¤ì •
- **ì—í­**: 2 (ê¸°ë³¸ê°’)
- **ë°°ì¹˜ í¬ê¸°**: 4 (per device)
- **í•™ìŠµë¥ **: 2e-4
- **ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´**: 1536
- **LoRA**: r=16, alpha=32
- **ì–‘ìí™”**: 4-bit QLoRA

### 5. ëª¨ë¸ ì¶”ë¡ 
```bash
# ê¸°ë³¸ ì¶”ë¡ 
python src/inference.py --adapter src/finetuned/checkpoint-694

# ìƒ˜í”Œ ë°ì´í„° í…ŒìŠ¤íŠ¸
python src/inference.py --adapter src/finetuned/checkpoint-694 --sample

# ë² ì´ìŠ¤ ëª¨ë¸ ì‚¬ìš©
python src/inference.py --base
```

### 6. ëª¨ë¸ ìƒ˜í”Œ í‰ê°€ 

- **src/submission/evaluation.py script**ë¡œ BLEUìŠ¤ì½”ì–´ê°€ ì•„ë‹ˆë¼ 5ê°œì˜ ë‹¤ë¥¸ ìŠ¤ì½”ì–´ ì ìˆ˜ë¥¼ í™•ì¸ ê°€ëŠ¥í•©ë‹ˆë‹¤. 
- ë¨¼ì € Ground Truth ì—­í• ì„ í•˜ëŠ” GT.csv íŒŒì¼ì„ sample datasetìœ¼ë¡œë¶€í„° ìˆ˜ë™ìœ¼ë¡œ ì œì‘í•´ì•¼í•©ë‹ˆë‹¤.

```bash
python src/inference.py --adapter src/finetuned/checkpoint-694

pythonn src/submission/evaluation.py --submission src/submission/finetuned_sample_submission.csv --gt src/submission/GT.csv
```

## Task

1. **summarization**
2. **math_reasoning**
3. **captioning** 
4. **vqa** 
5. **text_qa**

## Architecture

- **ë² ì´ìŠ¤ ëª¨ë¸**: Qwen2.5-VL-7B-Instruct
- **íŒŒì¸íŠœë‹**: QLoRA (4-bit ì–‘ìí™”)
- **ì–´ëŒ‘í„°**: LoRA (Low-Rank Adaptation)
- **í›ˆë ¨ ë°©ì‹**: Supervised Fine-Tuning (SFT)

- í›ˆë ¨ ê²°ê³¼ëŠ” `training_metrics.csv`ì™€ `training_metrics.png`ë¡œ ì €ì¥ë©ë‹ˆë‹¤.

## ì£¼ì˜ì‚¬í•­

1. **í˜„ì¬ ë²„ì „ì€ í…ìŠ¤íŠ¸ í•™ìŠµ ì „ìš© ëª¨ë“œ**ë¡œ ê°œë°œë˜ì–´ ìˆê³  multimodal ëª¨ë“œëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ 
2. **GPU ë©”ëª¨ë¦¬**: 24GB ì´ìƒ GPU ê¶Œì¥ 

## License

ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„ ìŠ¤ í•˜ì— ë°°í¬ë©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [LICENSE](LICENSE) íŒŒì¼ì„ ì°¸ì¡°í•˜ì„¸ìš”.



