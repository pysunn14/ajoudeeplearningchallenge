#!/usr/bin/env python3

"""
evaluation.py
ì¢…í•© ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ í‰ê°€ 

- Public Score: ì „ì²´ BLEU í‰ê· 
- Private Score: íƒœìŠ¤í¬ë³„ ë©”íŠ¸ë¦­ í‰ê·  

  * Math Reasoning: Exact Match
  * Captioning: CLIP Score  
  * VQA: Accuracy
  * Text QA: Accuracy
  * Summarization: BERT Score
  
python evaluation.py --submission submission.csv --gt GT.csv

"""
import os
import sys

os.environ['HOME'] = '/hdd1/minseok'
os.environ['HF_HOME'] = '/hdd1/minseok/dev/hf_cache'
os.environ['TRANSFORMERS_CACHE'] = '/hdd1/minseok/dev/hf_cache/transformers'
os.environ['HF_DATASETS_CACHE'] = '/hdd1/minseok/dev/hf_cache/datasets'
os.environ['TORCH_HOME'] = '/hdd1/minseok/dev/hf_cache/torch'

os.environ['CLIP_CACHE_DIR'] = '/hdd1/minseok/dev/hf_cache/models--openai--clip-vit-base-patch32'
os.environ['XDG_CACHE_HOME'] = '/hdd1/minseok/dev/hf_cache'

os.makedirs('/hdd1/minseok/dev/hf_cache/transformers', exist_ok=True)
os.makedirs('/hdd1/minseok/dev/hf_cache/datasets', exist_ok=True)
os.makedirs('/hdd1/minseok/dev/hf_cache/torch', exist_ok=True)
os.makedirs('/hdd1/minseok/dev/hf_cache/clip', exist_ok=True)

import pandas as pd
import argparse
from collections import Counter
import math
import re
import string
from typing import List, Tuple, Dict, Optional
import json
import warnings
import ast
from PIL import Image
import requests
from io import BytesIO
import base64
import numpy as np
import hashlib
import traceback

# === ì´ë¯¸ì§€ ìºì‹œ ê´€ë ¨ ì„¤ì • (inference.pyì—ì„œ ê°€ì ¸ì˜´) ===
FILE_PATH = '/hdd1/minseok/dev/contest/multimodal/'
IMAGE_CACHE_DIR = os.path.join(FILE_PATH, "image_cache")
DOWNLOAD_REPORT_PATH = os.path.join(FILE_PATH, "download_report")

def _url_to_cache_path(url: str) -> str:
    parsed = url.split("?")[0]
    ext = os.path.splitext(parsed)[1] if "." in parsed else ".jpg"
    name = hashlib.sha1(url.encode()).hexdigest()
    return os.path.join(IMAGE_CACHE_DIR, name + ext)

def load_successful_urls():
    successful_urls_path = os.path.join(DOWNLOAD_REPORT_PATH, "successful_urls.txt")
    
    if not os.path.exists(successful_urls_path):
        return set()
    
    successful_urls = set()
    with open(successful_urls_path, "r", encoding="utf-8") as f:
        for line in f:
            url = line.strip()
            if url:
                successful_urls.add(url)
    
    print(f" ë‹¤ìš´ë¡œë“œ ì„±ê³µí•œ URL: {len(successful_urls)}ê°œ") 
    return successful_urls

def download_image_from_url(url: str, timeout: int = 10):
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, timeout=timeout, headers=headers)
        response.raise_for_status()
        
        # ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì—ì„œ PILë¡œ ì—´ê¸°
        image_data = response.content
        image = Image.open(BytesIO(image_data)).convert("RGB")
        
        return image
    except Exception as e:
        print(f"HTTP ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ {url[:50]}...: {e}")
        return None

def load_image_from_cache_or_download(input_data, successful_urls=None, enable_download=True):
    try:
        if isinstance(input_data, str) and input_data.startswith("http"):
            # ìºì‹œ í™•ì¸
            if successful_urls and input_data in successful_urls:
                cache_path = _url_to_cache_path(input_data)
                if os.path.exists(cache_path):
                    return Image.open(cache_path).convert("RGB"), "cache_hit"

            # ìºì‹œì— ì—†ìœ¼ë©´ HTTP ë‹¤ìš´ë¡œë“œ ì‹œë„
            if enable_download:
                image = download_image_from_url(input_data)
                if image is not None:
                    return image, "http_download"
            
            # ëª¨ë“  ë°©ë²• ì‹¤íŒ¨
            return None, "cache_miss"
                
        elif isinstance(input_data, bytes):
            return Image.open(BytesIO(input_data)).convert("RGB"), "base64_bytes"
            
        elif isinstance(input_data, str):
            try:
                image_bytes = base64.b64decode(input_data)
                return Image.open(BytesIO(image_bytes)).convert("RGB"), "base64_string"
            except Exception:
                return None, "base64_error"
        else:
            return None, "unknown_format"
            
    except Exception as e:
        return None, f"error_{str(e)[:20]}"

os.environ['HF_HOME'] = '/hdd1/minseok/dev/hf_cache'
os.environ['TRANSFORMERS_CACHE'] = '/hdd1/minseok/dev/hf_cache/transformers'
os.environ['HF_DATASETS_CACHE'] = '/hdd1/minseok/dev/hf_cache/datasets'
os.makedirs('/hdd1/minseok/dev/hf_cache/transformers', exist_ok=True)
os.makedirs('/hdd1/minseok/dev/hf_cache/datasets', exist_ok=True)

# í‰ê°€ ë©”íŠ¸ë¦­ì„ ìœ„í•œ imports
try:
    import open_clip
    import torch
    import torch.nn.functional as F
    OPEN_CLIP_AVAILABLE = True
except ImportError:
    OPEN_CLIP_AVAILABLE = False
    warnings.warn("open_clip ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ì–´ CLIP Scoreë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# BERT Score ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‹œë„ 
try:
    from bert_score import score as bert_score_func
    BERT_SCORE_AVAILABLE = True
except ImportError:
    BERT_SCORE_AVAILABLE = False
    warnings.warn("bert-score ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ì–´ ìš”ì•½ë¬¸ í‰ê°€ê°€ ì œí•œë©ë‹ˆë‹¤.")

# Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‹œë„
try:
    import transformers
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    warnings.warn("transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")

def calculate_bert_score(candidate: str, reference: str) -> Dict[str, float]:
    
    if not candidate or not reference:
        return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}
    
    if BERT_SCORE_AVAILABLE:
        try:
            # ë¡œì»¬ BERT ëª¨ë¸ ë””ë ‰í„°ë¦¬ ì„¤ì •
            local_bert_dir = "/hdd1/minseok/dev/contest/multimodal/model/bert"
            bert_model_path = os.path.join(local_bert_dir, "models--google-bert--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594")
            
            print(f"ğŸ” BERT ëª¨ë¸ ë””ë ‰í„°ë¦¬ í™•ì¸: {bert_model_path}")
            print(f"ğŸ” ê²½ë¡œ ì¡´ì¬ ì—¬ë¶€: {os.path.exists(bert_model_path)}")
            
            # í™˜ê²½ ë³€ìˆ˜ ë°±ì—…
            original_transformers_cache = os.environ.get('TRANSFORMERS_CACHE', None)
            original_hf_home = os.environ.get('HF_HOME', None)
            
            # local_cache 
            os.environ['TRANSFORMERS_CACHE'] = local_bert_dir
            os.environ['HF_HOME'] = local_bert_dir
            
            try:
                if os.path.exists(bert_model_path):
                    print(f"âœ… ë¡œì»¬ BERT ëª¨ë¸ ë°œê²¬")
                    
                    P, R, F1 = bert_score_func(
                        [candidate], [reference], 
                        lang="en", 
                        model_type="bert-base-uncased",  # í‘œì¤€ ëª¨ë¸ëª… ì‚¬ìš©
                        verbose=False,
                        device="cpu",
                        idf=False,  # IDF ê°€ì¤‘ì¹˜ ë¹„í™œì„±í™”ë¡œ ì†ë„ í–¥ìƒ
                        batch_size=1
                    )

                else:
                    print(f" ë¡œì»¬ BERT ëª¨ë¸ ì—†ìŒ, ë‹¤ìš´ë¡œë“œ") 
                    P, R, F1 = bert_score_func(
                        [candidate], [reference], 
                        lang="en", 
                        model_type="bert-base-uncased",
                        verbose=False,
                        device="cpu",
                        idf=False,
                        batch_size=1
                    )
                    print(f" BERT ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ") 
                
                print(f" BERT Score ê³„ì‚° ì™„ë£Œ: P={float(P[0]):.4f}, R={float(R[0]):.4f}, F1={float(F1[0]):.4f}")
                
                return {
                    'precision': float(P[0]),
                    'recall': float(R[0]),
                    'f1': float(F1[0])
                }
                
            finally:
                # í™˜ê²½ ë³€ìˆ˜ ë³µì›
                if original_transformers_cache is not None:
                    os.environ['TRANSFORMERS_CACHE'] = original_transformers_cache
                else:
                    os.environ.pop('TRANSFORMERS_CACHE', None)
                    
                if original_hf_home is not None:
                    os.environ['HF_HOME'] = original_hf_home
                else:
                    os.environ.pop('HF_HOME', None)
                    
        except Exception as e:
            warnings.warn(f"BERT Score ê³„ì‚° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
            print(f" ìƒì„¸ ì—ëŸ¬: {type(e).__name__}: {e}")
            import traceback
            traceback.print_exc()
            return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}

    else:
        print('BERT-score ì‚¬ìš©ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.')
        return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}

def calculate_clip_score(image_path_or_url: str, text: str) -> float:
    if not text or not image_path_or_url:
        return 0.0
    
    try:
        import open_clip
        import torch
        from PIL import Image
        import requests
        from io import BytesIO
        import base64
        
        class SimpleOpenCLIPScore:
            def __init__(self):
                # ë¡œì»¬ ëª¨ë¸ ê²½ë¡œ
                local_model_path = "/hdd1/minseok/dev/contest/multimodal/model/clip/models--laion--CLIP-ViT-B-32-laion2B-s34B-b79K/snapshots/1a25a446712ba5ee05982a381eed697ef9b435cf/"
                
                # ğŸ” ë””ë²„ê¹…: ê²½ë¡œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                print(f"ğŸ” ë””ë²„ê¹… - ëª¨ë¸ ê²½ë¡œ í™•ì¸: {local_model_path}")
                print(f"ğŸ” ë””ë ‰í„°ë¦¬ ì¡´ì¬ ì—¬ë¶€: {os.path.exists(local_model_path)}")
                print(f"ğŸ” ë””ë ‰í„°ë¦¬ì¸ì§€ í™•ì¸: {os.path.isdir(local_model_path)}")
                
                # ìƒìœ„ ë””ë ‰í„°ë¦¬ë„ í™•ì¸
                parent_dir = os.path.dirname(local_model_path)
                print(f"ğŸ” ìƒìœ„ ë””ë ‰í„°ë¦¬: {parent_dir}")
                print(f"ğŸ” ìƒìœ„ ë””ë ‰í„°ë¦¬ ì¡´ì¬: {os.path.exists(parent_dir)}")
                
                # ë””ë ‰í„°ë¦¬ ë‚´ íŒŒì¼ ëª©ë¡ í™•ì¸
                if os.path.exists(local_model_path): 

                    try:
                        files = os.listdir(local_model_path)
                        print(f"ğŸ” ë””ë ‰í„°ë¦¬ ë‚´ íŒŒì¼ë“¤: {files}")
                        # .bin íŒŒì¼ ì°¾ê¸°
                        bin_files = [f for f in files if f.endswith('.bin')]
                        safetensors_files = [f for f in files if f.endswith('.safetensors')]
                        print(f" .bin íŒŒì¼ë“¤: {bin_files}")
                        print(f" .safetensors íŒŒì¼ë“¤: {safetensors_files}")

                    except Exception as e:
                        print(f" ë””ë ‰í„°ë¦¬ ì½ê¸° ì‹¤íŒ¨: {e}")
                        
                # ì‹¤ì œ ëª¨ë¸ íŒŒì¼ ê²½ë¡œ ì°¾ê¸°
                possible_files = [
                    os.path.join(local_model_path, "open_clip_pytorch_model.bin"),
                    os.path.join(local_model_path, "pytorch_model.bin"),
                    os.path.join(local_model_path, "model.safetensors"),
                    os.path.join(local_model_path, "open_clip_pytorch_model.safetensors")
                ]
                
                actual_model_file = None
                for file_path in possible_files:
                    print(f" íŒŒì¼ í™•ì¸: {file_path} -> ì¡´ì¬: {os.path.exists(file_path)}")
                    if os.path.exists(file_path):
                        actual_model_file = file_path
                        break
                
                if not actual_model_file:
                    raise FileNotFoundError(f" ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í™•ì¸ëœ ê²½ë¡œ: {local_model_path}")
                
                print(f"âœ… ì‹¤ì œ ì‚¬ìš©í•  ëª¨ë¸ íŒŒì¼: {actual_model_file}")

                # ê°„ë‹¨í•˜ê²Œ torch.loadë¡œ ì§ì ‘ ë¡œë“œ
                self.model, _, self.preprocess = open_clip.create_model_and_transforms('ViT-B-32')
                
                # ë¡œì»¬ ê°€ì¤‘ì¹˜ ì§ì ‘ ë¡œë“œ
                state_dict = torch.load(actual_model_file, map_location='cpu')
                self.model.load_state_dict(state_dict)
                self.model.eval()
                
                print(f" OpenCLIP ë¡œì»¬ ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {actual_model_file}") 
                
            def score(self, image_path_or_url, text):
                try:
                    # ğŸ” ì´ë¯¸ì§€ ì…ë ¥ ë””ë²„ê¹…
                    print(f" ì´ë¯¸ì§€ ì…ë ¥ íƒ€ì…: {type(image_path_or_url)}")
                    print(f" ì´ë¯¸ì§€ ì…ë ¥ ê¸¸ì´: {len(str(image_path_or_url))}")
                    print(f" ì´ë¯¸ì§€ ì…ë ¥ ì‹œì‘ ë¶€ë¶„: {str(image_path_or_url)[:100]}...")
                    
                    if not hasattr(self, 'successful_urls'):
                        self.successful_urls = load_successful_urls()
                    
                    # ìºì‹œ ìš°ì„  ë¡œë”© ì‹œë„
                    image, load_status = load_image_from_cache_or_download(
                        image_path_or_url, self.successful_urls, enable_download=True
                    )
                    
                    print(f" ì´ë¯¸ì§€ ë¡œë”© ìƒíƒœ: {load_status}")
                    
                    if image is None:
                        print(f" ì´ë¯¸ì§€ ë¡œë”© ì‹¤íŒ¨: {load_status}")
                        return 0.0
                    
                    print(f" ì´ë¯¸ì§€ ë¡œë”© ì„±ê³µ: {image.size}, ë°©ë²•: {load_status}")
                    
                    # ì „ì²˜ë¦¬
                    image_input = self.preprocess(image).unsqueeze(0)
                    text_input = open_clip.tokenize([text])
                    
                    with torch.no_grad():
                        image_features = self.model.encode_image(image_input)
                        text_features = self.model.encode_text(text_input)
                        
                        # ì •ê·œí™”
                        image_features /= image_features.norm(dim=-1, keepdim=True)
                        text_features /= text_features.norm(dim=-1, keepdim=True)
                        
                        # ìœ ì‚¬ë„ ê³„ì‚°
                        similarity = (image_features @ text_features.T).item()
                    
                    # 0~1 ë²”ìœ„ë¡œ ì •ê·œí™”
                    score = max(0.0, min(1.0, (similarity + 1) / 2))
                    print(f" CLIP Score ê³„ì‚° ì™„ë£Œ : {score}")
                    return score
                    
                except Exception as e:
                    print(f" ì—ëŸ¬ : {type(e).__name__}: {e}")
                    import traceback
                    traceback.print_exc()
                    warnings.warn(f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    return 0.0
                    
        # ëª¨ë¸ ê°ì²´ ìƒì„± (ìºì‹±)
        if not hasattr(calculate_clip_score, "openclip_model"):
            calculate_clip_score.openclip_model = SimpleOpenCLIPScore()
        
        return calculate_clip_score.openclip_model.score(image_path_or_url, text)
        
    except Exception as e:
        warnings.warn(f"OpenCLIP Score ê³„ì‚° ì‹¤íŒ¨: {e}")
        return 0.0

def calculate_text_qa_word_accuracy(predicted_list: List[str], ground_truth_list: List[str]) -> float:
    if not predicted_list or not ground_truth_list:
        return 0.0
    
    total_words = len(ground_truth_list)
    if total_words == 0:
        return 0.0
    
    correct_words = 0
    
    # ê¸¸ì´ê°€ ë‹¤ë¥¸ ê²½ìš° ì§§ì€ ìª½ì— ë§ì¶¤
    min_length = min(len(predicted_list), len(ground_truth_list))
    
    for i in range(min_length):
        pred_word = str(predicted_list[i]).strip().lower()
        gt_word = str(ground_truth_list[i]).strip().lower()
        
        if pred_word == gt_word:
            correct_words += 1
    
    return correct_words / total_words

def calculate_summarization_bert_score(candidate: str, reference: str) -> float:

    bert_scores = calculate_bert_score(candidate, reference)
    return bert_scores['f1']

def parse_text_qa_ground_truth(ground_truth: str) -> List[str]:
    if not ground_truth:
        return []
    
    try:
        # JSON íŒŒì‹± ì‹œë„
        if ground_truth.strip().startswith('{'):
            try:
                data = json.loads(ground_truth)
            except json.JSONDecodeError:
                # Python ë”•ì…”ë„ˆë¦¬ ë¬¸ìì—´ 
                data = ast.literal_eval(ground_truth)
            
            if isinstance(data, dict) and 'input_text' in data:
                input_text = data['input_text']
                if isinstance(input_text, list):
                    return [str(item) for item in input_text]
                else:
                    return [str(input_text)]
        
        # ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì§ì ‘ íŒŒì‹± ì‹œë„
        if ground_truth.strip().startswith('['):
            try:
                data = json.loads(ground_truth)
                if isinstance(data, list):
                    return [str(item) for item in data]
            except json.JSONDecodeError:
                data = ast.literal_eval(ground_truth)
                if isinstance(data, list):
                    return [str(item) for item in data]
        
        # ë‹¨ì¼ ë¬¸ìì—´ì¸ ê²½ìš°
        return [str(ground_truth)]
        
    except Exception as e:
        warnings.warn(f"Ground truth íŒŒì‹± ì‹¤íŒ¨: {e}")
        return [str(ground_truth)]

def parse_text_qa_prediction(prediction: str) -> List[str]:
    if not prediction:
        return []
    
    # ì½¤ë§ˆë¡œ êµ¬ë¶„ëœ raw textë¥¼ íŒŒì‹±
    answers = [answer.strip() for answer in prediction.split(',')]
    return [answer for answer in answers if answer]  # ë¹ˆ ë¬¸ìì—´ ì œê±°

def calculate_vqa_accuracy(candidate: str, reference: str) -> float:
    cand_normalized = normalize_text(candidate)
    ref_normalized = normalize_text(reference)
    
    return 1.0 if cand_normalized == ref_normalized else 0.0

def normalize_text(text: str) -> str:
    if pd.isna(text) or text is None:
        return ""
    
    text = str(text).lower()
    # êµ¬ë‘ì  ì œê±°
    text = text.translate(str.maketrans('', '', string.punctuation))
    # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def tokenize(text: str) -> List[str]:
    normalized = normalize_text(text)
    return normalized.split() if normalized else []

def extract_math_answer(text: str) -> str:
    if pd.isna(text) or text is None:
        return ""
    
    text = str(text)
    # #### íŒ¨í„´ ì°¾ê¸° (í•´ì‹œ 4ê°œ ë’¤ì˜ ìˆ«ì/ë¬¸ì)
    pattern = r'####\s*([^\s\n]+)'
    matches = re.findall(pattern, text)
    
    if matches:
        # ë§ˆì§€ë§‰ #### ë’¤ì˜ ë‹µì•ˆ ì‚¬ìš©
        answer = matches[-1].strip()
        # ìˆ«ìë§Œ ì¶”ì¶œ (ì†Œìˆ˜ì , ìŒìˆ˜ í¬í•¨)
        number_pattern = r'^-?\d+\.?\d*$'
        if re.match(number_pattern, answer):
            return answer
        else:
            # ìˆ«ìê°€ ì•„ë‹Œ ê²½ìš° ì›ë³¸ ë°˜í™˜
            return answer
    
    return ""

def calculate_exact_match(prediction: str, reference: str) -> bool:
    pred_answer = extract_math_answer(prediction)
    ref_answer = extract_math_answer(reference)
    
    if not pred_answer or not ref_answer:
        return False
    
    # ìˆ«ìì¸ ê²½ìš° ìˆ˜ì¹˜ ë¹„êµ
    try:
        pred_num = float(pred_answer)
        ref_num = float(ref_answer)
        # ë¶€ë™ì†Œìˆ˜ì  ë¹„êµ (ì‘ì€ ì˜¤ì°¨ í—ˆìš©)
        return abs(pred_num - ref_num) < 1e-6
    except ValueError:
        # ìˆ«ìê°€ ì•„ë‹Œ ê²½ìš° ë¬¸ìì—´ ë¹„êµ
        return pred_answer.lower() == ref_answer.lower()

def calculate_ngram_precision(candidate_tokens: List[str], reference_tokens: List[str], n: int) -> float:

    if len(candidate_tokens) < n:
        return 0.0
    
    # candidateì—ì„œ n-gram ìƒì„±
    candidate_ngrams = []
    for i in range(len(candidate_tokens) - n + 1):
        ngram = tuple(candidate_tokens[i:i+n])
        candidate_ngrams.append(ngram)
    
    # referenceì—ì„œ n-gram ìƒì„±
    reference_ngrams = []
    for i in range(len(reference_tokens) - n + 1):
        ngram = tuple(reference_tokens[i:i+n])
        reference_ngrams.append(ngram)
    
    if not candidate_ngrams:
        return 0.0
    
    # n-gram ì¹´ìš´íŠ¸
    candidate_counts = Counter(candidate_ngrams)
    reference_counts = Counter(reference_ngrams)
    
    # í´ë¦¬í•‘ëœ ì¹´ìš´íŠ¸ (referenceì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” ìµœëŒ€ íšŸìˆ˜ë¡œ ì œí•œ)
    clipped_counts = 0
    total_counts = len(candidate_ngrams)
    
    for ngram, count in candidate_counts.items():
        clipped_counts += min(count, reference_counts.get(ngram, 0))
    
    return clipped_counts / total_counts if total_counts > 0 else 0.0

def calculate_brevity_penalty(candidate_length: int, reference_length: int) -> float:
    
    if candidate_length > reference_length:
        return 1.0
    elif candidate_length == 0:
        return 0.0
    else:
        return math.exp(1 - reference_length / candidate_length)

def calculate_bleu_score(candidate: str, reference: str, max_n: int = 4) -> Tuple[float, dict]:

    candidate_tokens = tokenize(candidate)
    reference_tokens = tokenize(reference)
    
    if not candidate_tokens or not reference_tokens:
        return 0.0, {"1-gram": 0.0, "2-gram": 0.0, "3-gram": 0.0, "4-gram": 0.0, "BP": 0.0}
    
    # n-gram precision ê³„ì‚°
    precisions = []
    precision_details = {}
    
    for n in range(1, max_n + 1):
        precision = calculate_ngram_precision(candidate_tokens, reference_tokens, n)
        precisions.append(precision)
        precision_details[f"{n}-gram"] = precision
    
    # Brevity Penalty ê³„ì‚°
    bp = calculate_brevity_penalty(len(candidate_tokens), len(reference_tokens))
    precision_details["BP"] = bp
    
    # BLEU ìŠ¤ì½”ì–´ ê³„ì‚° (ê¸°í•˜í‰ê· )
    if any(p == 0 for p in precisions):
        bleu_score = 0.0
    else:
        log_precisions = [math.log(p) for p in precisions]
        bleu_score = bp * math.exp(sum(log_precisions) / len(log_precisions))
    
    return bleu_score, precision_details

def load_and_validate_data(submission_path: str, gt_path: str) -> Tuple[pd.DataFrame, pd.DataFrame]:

    print(f" Submission: {submission_path}")
    print(f" Ground Truth: {gt_path}")
    
    if not os.path.exists(submission_path):
        raise FileNotFoundError(f"Submission íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {submission_path}")
    
    if not os.path.exists(gt_path):
        raise FileNotFoundError(f"Ground Truth íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {gt_path}")
    
    submission_df = pd.read_csv(submission_path)
    gt_df = pd.read_csv(gt_path)
    
    # GT ë°ì´í„°ì— id ì»¬ëŸ¼ ì¶”ê°€ (0ë¶€í„° ì‹œì‘)
    gt_df = gt_df.reset_index(drop=True)
    gt_df['id'] = gt_df.index
    
    print(f" [ ë°ì´í„° ë¡œë”© ì™„ë£Œ ] ")
    
    return submission_df, gt_df

def calculate_metrics(submission_df: pd.DataFrame, gt_df: pd.DataFrame) -> dict:
    
    merged_df = pd.merge(submission_df, gt_df, on='id', suffixes=('_pred', '_gt'))
    
    if len(merged_df) == 0:
        raise ValueError("ë§¤ì¹­ë˜ëŠ” ìƒ˜í”Œ ì—†ìŒ. ")
        
    print(f"ë§¤ì¹­ ìƒ˜í”Œ: {len(merged_df)}ê°œ")
    
    individual_results = []
    task_scores = {}
    all_bleu_scores = [] 
    
    evaluation_summary = {
        'captioning': 'CLIP Score ',
        'vqa': 'Accuracy ',
        'text_qa': 'Word-level Accuracy ',
        'math_reasoning': 'Exact Match ',
        'summarization': 'BERT Score F1 '
    }
    
    for idx, row in merged_df.iterrows():
        candidate = row['output_pred']
        reference = row['output_gt']
        task = row['task']
        input_data = row.get('input', '')
        
        # 1. Public Scoreìš© BLEU ê³„ì‚° (ëª¨ë“  íƒœìŠ¤í¬)
        bleu_score, _ = calculate_bleu_score(candidate, reference)
        all_bleu_scores.append(bleu_score)
        
        # 2. Private Scoreìš© íƒœìŠ¤í¬ë³„ ì „ë¬¸ ë©”íŠ¸ë¦­
        primary_score = 0.0
        metric_type = 'unknown'
        additional_info = {}
        
        if task == 'captioning':
            if input_data:  # ì´ë¯¸ì§€ URL/ê²½ë¡œê°€ ìˆëŠ” ê²½ìš°
                primary_score = calculate_clip_score(input_data, candidate)
                metric_type = 'clip_score'
            else:
                primary_score = bleu_score
                metric_type = 'bleu_fallback'
                
        elif task == 'vqa':
            primary_score = calculate_vqa_accuracy(candidate, reference)
            metric_type = 'vqa_accuracy'
            
        elif task == 'text_qa':
            pred_words = parse_text_qa_prediction(candidate)  # ì½¤ë§ˆ 
            gt_words = parse_text_qa_ground_truth(reference)  # JSON ë”•ì…”ë„ˆë¦¬
            primary_score = calculate_text_qa_word_accuracy(pred_words, gt_words)
            metric_type = 'word_accuracy'
            additional_info = {
                'pred_word_count': len(pred_words),
                'gt_word_count': len(gt_words)
            }
            
        elif task == 'math_reasoning':
            # Exact Match
            primary_score = calculate_exact_match(candidate, reference)
            metric_type = 'exact_match'
            additional_info = {
                'pred_answer': extract_math_answer(candidate),
                'gt_answer': extract_math_answer(reference)
            }
            
        elif task == 'summarization':
            # BERT Score - ì›ë³¸ ìš”ì•½ë¬¸ inputê³¼ ëª¨ë¸ ì¶œë ¥ ë¹„êµ
            original_input = row.get('input', '')  # ì›ë³¸ ìš”ì•½ë¬¸
            if original_input:
                primary_score = calculate_summarization_bert_score(candidate, original_input)
                comparison_target = 'original_input'
            else:
                # inputì´ ì—†ëŠ” ê²½ìš° ê¸°ì¡´ ë°©ì‹ ì‚¬ìš© (í•˜ìœ„ í˜¸í™˜ì„±)
                primary_score = calculate_summarization_bert_score(candidate, reference)
                comparison_target = 'ground_truth'
            
            metric_type = 'bert_score_f1'
            additional_info = {'comparison_target': comparison_target}
            
        else:
            # ì•Œ ìˆ˜ ì—†ëŠ” íƒœìŠ¤í¬ëŠ” BLEU ì‚¬ìš©
            primary_score = bleu_score
            metric_type = 'bleu_fallback'
        
        # ê°œë³„ ê²°ê³¼ ì €ì¥
        individual_results.append({
            'id': row['id'],
            'task': task,
            'primary_score': primary_score,
            'bleu_score': bleu_score,
            'metric_type': metric_type,
            'additional_info': additional_info
        })
        
        # íƒœìŠ¤í¬ë³„ ì ìˆ˜ ëˆ„ì 
        if task not in task_scores:
            task_scores[task] = []
        task_scores[task].append(primary_score)
    
    # Public Score
    public_score = sum(all_bleu_scores) / len(all_bleu_scores) if all_bleu_scores else 0.0
    
    # Private Score
    task_averages = {}
    task_means = []
    
    for task, scores in task_scores.items():
        if scores:
            mean_score = sum(scores) / len(scores)
            task_averages[task] = {
                'count': len(scores),
                'mean_score': mean_score,
                'min_score': min(scores),
                'max_score': max(scores),
                'metric_type': evaluation_summary.get(task, 'unknown')
            }
            task_means.append(mean_score)
    
    private_score = sum(task_means) / len(task_means) if task_means else 0.0
    
    return {
        'public_score': public_score,
        'private_score': private_score,
        'overall_bleu_reference': public_score,  # í˜¸í™˜ì„±
        'task_averages': task_averages,
        'individual_results': individual_results,
        'total_samples': len(merged_df),
        'evaluation_summary': evaluation_summary,
        'task_distribution': {task: len(scores) for task, scores in task_scores.items()}
    }

def print_results(results: dict):
    print(f"\n" + "="*80)
    print(f"[ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ í‰ê°€ ê²°ê³¼ ]")
    print(f"="*80)
    
    print(f"[ ì „ì²´ ì„±ëŠ¥ ìš”ì•½ ]") 
    print(f"  - ì „ì²´ í‰ê°€ ìƒ˜í”Œ: {results['total_samples']:,}ê°œ")
    print(f"  - ì°¸ê³ ìš© ì „ì²´ BLEU: {results['overall_bleu_reference']:.4f}")
    
    print(f"í‰ê°€ ì²´ê³„:") 
    
    for task, description in results['evaluation_summary'].items():
        if task in results['task_averages']:
            print(f"  - {task}: {description}")
            
    print(f" íƒœìŠ¤í¬ë³„ ìƒì„¸ ì„±ëŠ¥: ")
    for task, stats in results['task_averages'].items():
        print(f"\n  ğŸ”¹ {task}:")
        print(f"    â”œ ìƒ˜í”Œ ìˆ˜: {stats['count']}ê°œ")
        print(f"    â”œ í‰ê°€ ë©”íŠ¸ë¦­: {stats['metric_type']}")
        print(f"    â”œ í‰ê·  ì ìˆ˜: {stats['mean_score']:.4f}")
        print(f"    â”œ ìµœê³  ì ìˆ˜: {stats['max_score']:.4f}")
        print(f"    â”” ìµœì € ì ìˆ˜: {stats['min_score']:.4f}")
        
        mean_score = stats['mean_score']
        if mean_score >= 0.8:
            grade = "ğŸŸ¢"
        elif mean_score >= 0.6:
            grade = "ğŸŸ¡"
        elif mean_score >= 0.4:
            grade = "ğŸŸ "
        else:
            grade = "ğŸ”´"
        
        print(f" ì„±ëŠ¥ ë“±ê¸‰: {grade} ({mean_score:.3f})")
    
    # ì „ì²´ ì„±ëŠ¥ ì¢…í•© í‰ê°€
    overall_scores = [stats['mean_score'] for stats in results['task_averages'].values()]
    if overall_scores:
        overall_mean = sum(overall_scores) / len(overall_scores)
        if overall_mean >= 0.7:
            overall_grade = "ğŸŸ¢ "
        elif overall_mean >= 0.5:
            overall_grade = "ğŸŸ¡ "
        elif overall_mean >= 0.3:
            overall_grade = "ğŸŸ  "
        else:
            overall_grade = "ğŸ”´ "
        
        print(f" [ ì¢…í•© í‰ê°€ ] {overall_grade}")
        print(f" í‰ê·  ì„±ëŠ¥ ì ìˆ˜: {overall_mean:.4f}")
        print(f" BLEU: {results['overall_bleu_reference']:.4f}")
    
    # ìµœì¢… ì ìˆ˜ ê°•ì¡° ì¶œë ¥
    print(f"\n" + "="*80)
    print(f"[ ìµœì¢… í‰ê°€ ì ìˆ˜ ]")
    print(f"="*80)
    print(f"Public Score (BLEU): {results.get('public_score', 0.0):.4f}")
    print(f"Private Score (Task Average): {results.get('private_score', 0.0):.4f}")
    print(f"="*80)

def save_detailed_results(results: dict, output_path: str = "evaluation_results.json"):
    
    # JSON 
    serializable_results = {
        'evaluation_summary': results['evaluation_summary'],
        'task_averages': results['task_averages'],
        'overall_bleu_reference': results['overall_bleu_reference'],
        'total_samples': results['total_samples'],
        'individual_results': results['individual_results'][:100],  # ì²˜ìŒ 100ê°œë§Œ ì €ì¥
        'metadata': {
            'evaluation_type': 'multimodal',
            'metrics_used': {
                'summarization': 'BERT Score (F1)',
                'math_reasoning': 'Exact Match',
                'captioning': 'CLIP Score',
                'vqa': 'Accuracy',
                'text_qa': 'Accuracy',
                'reference_bleu': 'BLEU (ëª¨ë“  íƒœìŠ¤í¬)'
            },
            'libraries_available': {
                'transformers': TRANSFORMERS_AVAILABLE,
                'bert_score': BERT_SCORE_AVAILABLE
            }
        }
    }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(serializable_results, f, indent=2, ensure_ascii=False)
        
    print(f" [ ì €ì¥ ì™„ë£Œ ]")

def main():
    parser = argparse.ArgumentParser(description='ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ í‰ê°€ (BERT Score, CLIP Score, Exact Match, Accuracy)')

    parser.add_argument('--submission', type=str, default='finetuned_submission_sample.csv')
    parser.add_argument('--gt', type=str, default='GT.csv')
    parser.add_argument('--output', type=str, default='evaluation_results.json')
    
    args = parser.parse_args()
    
    try:
        # ë°ì´í„° ë¡œë“œ ë° ê²€ì¦
        submission_df, gt_df = load_and_validate_data(args.submission, args.gt)
        
        # í‰ê°€ ìˆ˜í–‰
        results = calculate_metrics(submission_df, gt_df)
        
        # ê²°ê³¼ ì¶œë ¥
        print_results(results)
        
        # ê²°ê³¼ ì €ì¥
        if not args.no_save:
            save_detailed_results(results, args.output)

        print(f"ì´ {results['total_samples']} sample, {len(results['task_averages'])} task metric eval")

    except Exception as e:
        print(f"ì˜¤ë¥˜ : {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
        print(f"ì˜¤ë¥˜ : {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()